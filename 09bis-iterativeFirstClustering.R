## %######################################################%##
#                                                          #
####             This scripts performs the              ####
####           first clistering of hits into            ####
####        "communities" according to criteron         ####
####              1 of the study's method               ####
#                                                          #
## %######################################################%##

# This scripts is launched from 09-hitClusteringRound1.R and uses files generated by that script

library(data.table)
library(Biostrings)
library(dplyr)
library(stringi)
library(stringr)
library(igraph) #for function graph_from_data_frame
library(ape)
library(parallel)

path = "~/Project/"
setwd(path) 

source("HTvFunctions.R")

# read arguments
args <- commandArgs(trailingOnly = TRUE)

# job integer identifier(s) that specifies which super family + mrca to process
# these may be separated by comas
job <- as.integer(unlist(strsplit(args[1], ","))) #test avec job=22
nCPUs <- as.integer(args[2])

print(paste("job:", job))


# STEP ONE, we determine which hits to cluster according to the job identifier ------------------------------------------------------

# imports the the list of tables of hits (with conversion of copy names to integers)
hitList <- readRDS("TEs/clustering/round1/hitsForFirstClusteringdS05occ200.RDS")

nrow(rbindlist(hitList)) #4603510 < nrow(httHits) because we didn't keep hits which are alone in a group (since can't be clusterered with anyone)

# and the table we generated to indicate which hits to process
batches <- fread("TEs/clustering/round1/batchesdS05occ200.txt")
# the superfamily(ies) + mrca to process in this batch

#Make variables that will allow us to read the right files

#The job(s) invovle this (or these) superF and this (or these) mrca
superF <- batches[batch %in% job, superF] #Note: use %in% because one can give list of jobs
mrca <- batches[batch %in% job, mrca]

#But that's not all. We need to get the groups
  
#Get the list of all the combinaision of superF-mrca in list hitList
superAll <- splitToColumns(names(hitList), "_", column=1)
mrcaAll <- sapply(hitList, function(x){unique(x$mrca)})
superF_mrcaAll <- stri_c(superAll, mrcaAll, sep=' ')

#Get the combinaision we will process in those batch
superF_mrca <- batches[batch %in% job, stri_c(superF, mrca, sep=' ')]

#names(hitList) contains the name of each group. We retain only those belonging to the combinaision of superF-mrca that will be processed in this batch
groups <- names(hitList)[superF_mrcaAll %chin% superF_mrca]


# STEP TWO, we import the corresponding self-blast hits of copies within clades ----------------------------------------------------------------
#This is the output of the script 08. Make sure the path is correct, since I made quite a lot of modifications in scipt 08
blastFiles <- stri_c("TEs/clustering/selfBlastn_perClade/out/mrca_", mrca, "/", sub(".*[.]", "", superF), "_selfBlastn.out") 

#If there is a missing file or empty one, print it. I might want to cancell the job to solve it
if(length(blastFiles[!file.exists(blastFiles)])>0){
    print("Warning: these files do not exist: ")
    blastFiles[!file.exists(blastFiles)]
}

if(length(blastFiles[file.size(blastFiles) == 0])>0){    
    print("Warning: These files are empty: ")
    blastFiles[file.size(blastFiles) == 0]
}
blastFiles <- blastFiles[file.exists(blastFiles)]
blastFiles <- blastFiles[file.size(blastFiles) > 0]


blast <- rbindlist(lapply(X = blastFiles, FUN = function(file) {
    fread(
        input = file,
        header = F,
        select = c(1,2,3), 
        col.names = c("query", "subject", "pID"),
        sep = "\t"
    )
}))

# we convert pIDs to integer as we did for the htt hits (for memory and speed)
blast[, pID := as.integer(pID * 1000L)]



# STEP THREE, clustering of according to "criterion 1" --------------------------------------------------------------------------------
# this function does the clustering per 'group' (hits of pair of clades in a super family)
hitCommunities <- function(group) {

    # we first retrieve the hits to cluster. We copy the data table to
    # avoid some side effect related to how data.table functions work
    hits <- copy(hitList[[group]])

    # we obtain the different copies (integer ids) in these hits
    ucopies <- hits[, sort(unique(c(query, subject)))]

    # we convert these to smaller integers so that we can make matrices of blast pID
    # between copies, where a copy id will be a row/column index in the matrix
    hits[, c("qid", "sid") := .(
        match(query, ucopies),
        match(subject, ucopies)
    )]

    # we thus need to use the same ids for the copies in the blast output
    blast[, c("qid", "sid") := .(
        match(query, ucopies),
        match(subject, ucopies)
    )]

    # we extract the hits involving these copies
    selectedHits <- blast[!is.na(qid) & !is.na(sid)]

    # we make a matrix of pID for each copy pair (with pID zero by default, when there is no hit)
    pIDmatrix <- matrix(0L, length(ucopies), length(ucopies))

    # the diagonal is set to 100% pID (*1000), to represent perfect pID for a copy with itself
    diag(pIDmatrix) <- 100000L

    # we fill the matrix, (both semi matrices)
    pIDmatrix[cbind(selectedHits$qid, selectedHits$sid)] <- selectedHits$pID
    pIDmatrix[cbind(selectedHits$sid, selectedHits$qid)] <- selectedHits$pID

    # we cannot always cluster all the hits at once due to the max size of vectors in R (and RAM required)
    # so we get the number of hits to cluster
    nHits <- nrow(hits)

    # and determine the number of batches of hits, since we will have to make all possible pairs of hits
    nBatches <- ceiling(nHits^2 / 2^28)

    # in the following, a hit corresponds to a row index in the "hits" table
    # we split the hits into several batches 
    hitBatches <- splitEqual(1:(nHits - 1L), n = nBatches)


    # we "connect" hits 2 by 2 according to criterion 1, with this function:
    criterion_1 <- function(batch) {

        # for a batch of hits, we make all possible pairs of hits.
        # The left-hand hit (hit1) is from the batch, and the right-hand
        # hit (hit2) includes all other hits (including other batches)
        # this ensures that, over all batches, all possible pairs will be made
        pairs <- data.table(
            hit1 = rep(batch, nHits - batch),
            hit2 = unlist(x = lapply(
                X = batch[1]:max(batch) + 1L,
                FUN = function(hit) hit:nHits
            ))
        )

        # we retrieve the ids of copies involved in the 2 hits (2 per clade)
        pairs[, c("q1", "s1", "q2", "s2") := data.table(
            hits[hit1, .(qid, sid)],
            hits[hit2, .(qid, sid)]
        )]

        # and retrieve the blast pIDs (percentage identity) within each clade (intra) of of the 2 hits (inter)
        pairs[, c(
            "inter1", # between-clade pID, representing the HTT (for hit1)
            "inter2", # same for the right-hand hit1
            "intra1", # pID of copies within the left-clade (clade A)
            "intra2"
        ) # and for the right-clade
        := data.table(
                hits[hit1, pID],
                hits[hit2, pID],
                pIDmatrix[cbind(q1, q2)],
                pIDmatrix[cbind(s1, s2)]
            )]

        # to apply criterion 1, we get the highest within clade identity
        pairs[, maxIntra := pmax(intra1, intra2)]
        cat("*") # progress indicator (this can be long)

        # and we finally apply criterion 1 to "connect" the hits
        # in effect, we return pairs of hits where the best intra-clade
        # identity is higher than at least inter-clade identity (that of hits)
        #ATTENTION I should replace < by <=. Otherwise an inter hit of 100% (copy 1 vs 2) will not be connected to any hit, even if it involve the same copy (copy 1 vs 3)
        pairs[inter1 <= maxIntra | inter2 <= maxIntra, maxIntra, .(hit1, hit2)]
    }

    # we apply the function for batches of hit pairs and concatenate the results
    pairs <- rbindlist(lapply(hitBatches, criterion_1))
    
    #Check how many hits are returned. A hit returned has at least 1 connection with another hit
    nbHitsReturned <- length(unique(c(pairs$hit1, pairs$hit2)))

    # we now do the clustering if there are "connected hits"
    #NOTE: the algo cluster_fast_greedy has a weird behavious:
        #if everything is connected with everything, it  will always put the last node in another com 
        #(whereas everything should be together)
        
    #If pairs not empty, it means at least one connection
    if (nrow(pairs) > 0) {

        #and if not everything connected to everything
        if ((nbHitsReturned*nbHitsReturned-nbHitsReturned)/2 != nrow(pairs) ) { 

            cls <- graph_from_data_frame(pairs, directed = F)

            # we cluster into "communities" done with clauset et al. algorithm
            cls <- data.tableFromCommunities(cluster_fast_greedy(cls))
        
        #if everything is connected (in pairs), do not use cluster_fast_greedy
        #instead, put everything in the same com
        } else {
            cls <- data.table(member = unique(c(pairs$hit1, pairs$hit2)), community = 1)
        }
        
        cat("-") # to monitor progress

        # we attribute hits to "communities" named by integer numbers
        com <- integer(nrow(hits))
        com[cls$member] <- cls$community
        # so com[x] will give the community of hit "x" (which is an integer)

        # any hit that is not in a community now forms its own community
        com[com == 0L] <- 1:(sum(com == 0L)) + max(com)

        # we add the community column to the original table of htt hits
        hits[, comm := com]

    #if pairs is empty, it means that nothing is connected. So give one com per hit 
    #each hit gets its own community number corresponding to its row index   
    } else {
        hits[, comm := 1:.N]
    }

    # we write results to disk for safety (the function also returns the results)
    fwrite(
        hits[, .(hit, comm, group)],
        stri_c("TEs/clustering/round1/", group, ".groups.txt"),
        sep='\t'
    )

    hits[, .(hit, comm, group)]
}


# we apply the above function to the different groups of the super family, in parallel
res <- mclapply(
    X = groups,
    FUN = hitCommunities,
    mc.cores = nCPUs,
    mc.preschedule = F
)

print("binding all results...")

res <- rbindlist(res)
#This has the consequence to have the same id comm in different group,
#whereas totally intependant
# --> We take this into account in script 9 --> ucomm

# we write results to disk ---------------------------------------------------------------------
# there is one output file per job
# if the job processed several small super families, we use the super family name "other" in the output
if (length(unique(superF)) > 1) {
    superF <- "others"
} else {
  superF <- unique(superF)
}

if (length(unique(mrca)) > 1) {
    mrca <- "Any"
} else {
  mrca <- unique(mrca)
}

print("saving results...")

fwrite(res, stri_c("TEs/clustering/round1/", superF, "_mrca", mrca, ".allGroups.txt"), sep='\t')

print("finished")