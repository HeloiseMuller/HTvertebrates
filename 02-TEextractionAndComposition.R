


## %######################################################%##
#                                                          #
####         this stage  extracts TE copies from        ####
####          genomes using repeatMasker gffs           ####
#    and reports statistics on TE composition in species   #
#                                                          #
## %######################################################%##

# TEs have been characterised on each genome with repeat modeler
# and located with repeat masker.
# repeat modeler and repeat masker jobs were not automated in scripts
# because they were executed by other authors of the study

# example of repeatModeler job launched from bash, for one genome 
# (it is assumed that the working directory is specific to one species, and contains the species name in its path):
# RepeatModeler-open-1.0.10/BuildDatabase -engine ncbi -name mydb genome.fna		
# where genome.fna is a species genome (I believe it wouldn't worked on a gzipped fasta)
# RepeatModeler-open-1.0.10/RepeatModeler -engine ncbi -database mydb -pa 8 > run.out

# Then intermediate RepeatModeler files should be removed, as they are so numerous that they would slow the whole system:
# rm -rf RM_*/round-*

# example of repeatMasker job for one genome:
# RepeatMasker-open-4-0-7/RepeatMasker -nolow -no_is -norna -engine ncbi -parallel 1 -lib mydb-families.fa genome.fna
# where mydb-families.fa are TE consensus sequences generated from genome.fna


# This script uses the following data :
# - compressed genome sequences in a "genomes/" subfolder as the only files ending by ".fna.gz" 
# (though this script would work with incompressed fastas). These are generated by stage 1.

# - family consensus fasta files (in any location within the work directory) 
# these must be the only files ending by "families.fa" within the working directory and subdirectories

# - gff files corresponding to the repeatmasker TE annotations as the only files ending by ".gff"

# !! every file path must contain the name of a species corresponding exactly to a tip name of timetree.nwk

# the outputs are:
# - a gzipped fasta of TE copies for each species
# - text files reporting the number of copies per TE super family per genome
# - supplementary data 3 of the paper

source("HTvFunctions.R")

# preparing output folders -----------------------------------------------------------------------------

# folder were statistics on TE will be output
dir.create("TEs/TEcomposition", recursive = T)

# where TE copies will be exported
dir.create("TEs/copies")




# STEP ONE, we list the files we need for TE copy extraction ---------------------------------------------------------------------

# the gff files from repeat masker, which have TE copy coordinates
gff <- list.files(
    pattern = ".gff$",
    full.names = T,
    recursive = T
)

# we extract the species names from file paths and puts results in a table
gffs <- data.table(gff, sp = extractSpeciesNames(gff))

# we do the same for repeat modeler consensus sequence files:
cons <- list.files(
    pattern = "families.fa$",
    full.names = T,
    recursive = T
)

conss <- data.table(cons, sp = extractSpeciesNames(cons))

# and for the genome sequence files:
genome <- list.files(
    "genomes",
    pattern = "fna.gz$",
    full.names = T,
    recursive = T
)

genomes <- data.table(genome, sp = extractSpeciesNames(genome))


# as our function to extract copies will use all these files
# we put these file names in a table, where each row corresponds to a species ---------------------------------------
m <- merge(gffs, conss, by = "sp", all.x = T)
m <- merge(m, genomes, by = "sp")

# we add a column for the names of future output files 
# (compressed fastas of TE copies for each species)
m[, out := stri_c("TEs/copies/", sp, ".TEs.fasta.gz")]

# we avoid redoing some work in case the script needs to be relaunched
m <- m[!file.exists(out)]

# we will process bigger genomes first, not make better use of the CPUs
# (else, one single job may be running at the end for the largest genome)
sizes <- file.size(m$genome)
m <- m[order(sizes, decreasing = T)]


# we split the filename table by row (species) to send to the function below in parallel
# we don't call mcMap() on the table because it apparently has issues returning tables
fileList <- split(m, 1:nrow(m))




# STEP TWO, we extract TE copies and information on TE coposistion for different genomes in parallel -------------------------------------------------
# see extractCopies() function in HTvFunctions.R for details.

TEcomposition <- mclapply(
  X = fileList,
  FUN = function(files) do.call(extractCopies, files),
  mc.cores = 20,
  mc.preschedule = F
)

# we stack the data for all genomes in a single table
TEcomposition = rbindlist(TEcomposition)
writeT(TEcomposition, path = "TEs/TEcomposition/all.TEcomposition.txt")

# we generate the supplementary dataset associated with the paper
# in which we show the composition over all genomes on a per-superfamily basis

TEcompo <- TEcomposition[, .(
    number_of_copies_300bp = sum(nCopies),
    total_nucleotides = sum(bp),
    number_of_families = sum(nFam)
), by = .(species = sp, RepeatModeler_superfamily = superF)]


TEcompo <- TEcompo[number_of_copies_300bp > 0L]

writeT(TEcompo, "additional_files/supplementary-data3-TEcomposition_per_species.txt") 



