


##%######################################################%##
#                                                          #
####         this stages extracts TE copies from        ####
####          genomes using repeatMasker gffs           ####
#                                                          #
##%######################################################%##

### repeat modeler and repeat masker jobs were not automated in scripts (I didn't do this part)
### example of repeatModeler job launched from bash, for one genome (it is assumed that the working directory is specific to one species, and contains the species name in its path):
# RepeatModeler-open-1.0.10/BuildDatabase -engine ncbi -name mydb genome.fna		#where genome.fna is a species genome (I believe it wouldn't worked on a gzipped fasta)
# RepeatModeler-open-1.0.10/RepeatModeler -engine ncbi -database mydb -pa 8 > run.out

# Then intermediate RepeatModeler files should be removed, as they are so numerous that they would slow the whole system:
# rm -rf RM_*/round-*

# example of repeatMasker job for one genome:
# RepeatMasker-open-4-0-7/RepeatMasker -nolow -no_is -norna -engine ncbi -parallel 1 -lib mydb-families.fa genome.fna
# where mydb-families.fa are TE consensus sequences generated from genome.fna

### this script expects to find compressed genome sequences in a "genomes/" subfolder as the only files ending by ".fna.gz" (though this script would work with incompressed fastas). These should be generated by step 1. Also, the script expects:
### family consensus fasta files (in any location within the work directory) as the only files ending by "families.fa"
### and gff files corresponding to the repeatmasker TE annotations as the only files ending by ".gff"
### every file path must contain the name of a species corresponding exactly to a tip name of timetree.nwk

dir.create("TEs/TEcomposition", recursive = T)
dir.create("TEs/copies")

source("HTvFunctions.R")
gff = list.files(pattern = ".gff$",
                 full.names = T,
                 recursive = T)				#the gff files from repeat masker, listing TE copy coordinates
gffs = data.table(gff, sp = extractSpeciesNames(gff))							#extracts the species names from file paths and puts results in a table
cons = list.files(pattern = "families.fa$",
                  full.names = T,
                  recursive = T)		#does the same for repeat modeler consensus sequence files
conss = data.table(cons, sp = extractSpeciesNames(cons))
genome = list.files(
  "genomes",
  pattern = "fna.gz$",
  full.names = T,
  recursive = T
)		                                #and for the genome sequence files
genomes = data.table(genome, sp = extractSpeciesNames(genome))
m = merge(gffs, conss, by = "sp", all.x = T)
m = merge(m, genomes, by = "sp")		            #we put all these file names in a table (merged by species)
m[, out := stri_c("TEs/copies/", sp, ".TEs.fasta.gz")]			#names of future output files (compressed fastas of TE copies for each species)
m = m[!file.exists(out)]										  #in case the script needs to be relaunched, avoids redoing some work
sizes = file.size(m$genome)
m = m[order(sizes, decreasing = T)]				    #we will process bigger genomes first, not make better use of the CPUs

fileList = split(m, 1:nrow(m))						  #splits the table of file names by row, to send to the function below in parallel (I recall we don't call mcMap on the table because Map() apparently has issues returning tables)

TEcomposition = mclapply(fileList, function(files)
  do.call(extractCopies, files), mc.cores = 20, mc.preschedule = F)			#see extractCopies() function in HTvFunctions.R for details.
writeT(data = rbindlist(TEcomposition),
       path = "TEs/TEcomposition/all.TEcomposition.txt")
