## %######################################################%##
#                                                          #
####   This script finds homologies between TE copies   ####
####      belong to different hit groups, which we      ####
####        will use to infer the minimal number        ####
####       of HTT events to explain these groups        ####
#                                                          #
## %######################################################%##
# "homology" here refers to the existence of a blast hit

# this script is launched by step 12-HTTeventCount.R

source("HTvFunctions.R")

# the only argument is the number of CPU to use
args <- commandArgs(trailingOnly = TRUE)
nCPUs <- as.integer(args[1])

# the output will be a table that reports on the homology of copies between hit groups

dir.create("TEs/clustering/networks") # where results will go

# we import the list of hits (data tables with query id, subject id, pID, hit group id) in each super family
# this list was generated by 12-HTTeventCount.R
hitList <- readRDS("TEs/clustering/forHitGroupPairs.RDS")

# to find homologous copies, we use the self-blastn files generated at step 8
# we list them
blastFiles <- list.files(
    path = "TEs/clustering/blastn/done",
    pattern = ".out",
    full.names = T
)

# we split these file names by superfamily (large superfamilies have several blast files)
# for this we extract superfamily names from file names
superF <- splitToColumns(basename(blastFiles), "_", 1)
blastFiles <- split(blastFiles, superF)

# the core function that finds homologies between copies of different hit groups of a super family
hitGroupPairStats <- function(superF) { # superF is the superfamily name (character)


    hits <- hitList[[superF]] # we get the HTT hits of the super family being processed
    copies <- hits[, unique(c(q, s))] # and all the copies involved in these hits

    # first, we import blast files of copies against themseves, with this function
    import <- function(file) { # file is the path to a blast output
        selfBlast <- fread(
            input = file,
            sep = "\t",
            header = F,
            select = c(1:3),
            col.names = c("query", "subject", "pID"),
            nThread = 2
        )

        # we remove its not involving copies in the htt hits
        selfBlast <- selfBlast[query %in% copies & subject %in% copies]

        # and convert pID to integer for speed and RAM
        selfBlast[, pID := as.integer(pID * 1000)]
        selfBlast
    }

    # we apply the function in parallel for the blast files of the super family, using 5 processes
    selfBlast <- mclapply(
        X = blastFiles[[superF]],
        FUN = import,
        mc.cores = 5,
        mc.preschedule = F
    )

    selfBlast <- rbindlist(selfBlast) # we rbind the hits in a single table

    # we will need to quicky retreive all the copies that are homologous to a given one (including itself)
    # for this, we first duplicate the hits by creating reciprocal ones (used afetwards) and add self hits with 100% pIDs
    selfBlast <- rbind(
        selfBlast, # the original hits
        selfBlast[, .(query = subject, subject = query, pID)], # the hits in "reverse"
        data.table(query = copies, subject = copies, pID = 100000L)
    ) # and the self hits


    # we can now create the following list
    subjectsForQuery <- reList(split(selfBlast$subject, selfBlast$query))
    # subjectsForQuery[[x]] returns all subject were hit by query copy x (x is an integer)
    # since we have made all hits reciprocal, there is not difference between query and subject

    # we will also need to quickly retreive the pID score of all hits involving a query
    # the principle is the same as above
    idForQuery <- reList(split(selfBlast$pID, selfBlast$query))

    # as well as the hit group(s) in which each copy is found, following the same principle
    # but for this, we first have to obtain all ids of copies involved in each hit group
    copyPerHitGroup <- hits[, .(copy = unique(c(q, s))), by = hitGroup]
    hitGroupsForCopy <- reList(split(copyPerHitGroup$hitGroup, copyPerHitGroup$copy))

    copyPerHitGroup <- split(copyPerHitGroup$copy, copyPerHitGroup$hitGroup)

    # we count the number of subjects per query, used later
    nSubjects <- sapply(subjectsForQuery, length)

    # as well as the number of hit groups where each copy is found
    nHitGroups <- sapply(hitGroupsForCopy, length)

    rm(selfBlast) # to reclaim RAM, as there are lots of hits

    # for the copies of a given hit group ("focal" hit group), the function below
    # retreives all homologous copies and the hit groups to which they belong
    hitGroupLinks <- function(copies, focalHitGroup) { # focalHitGroup is just the integer id of the hit group

        # we firt retreive all copies that are homologous to those in the hit group (i.e., 'subject' of the blast)
        subject <- unlist(subjectsForQuery[copies])

        # with the pIDs associated to this homologies
        pID <- unlist(idForQuery[copies])

        # we place them in a data table (which requires replicating copies that are homologous to several subjects)
        pairs <- data.table(copy = rep(copies, nSubjects[copies]), subject, pID)

        # and we get the hit groups to which these subjects belongs
        explanatoryHitGroup <- unlist(hitGroupsForCopy[pairs$subject])

        # we will place all this information in a table
        # for this, we need the number of hitGroups in which the subjects are found
        len <- nHitGroups[pairs$subject]

        # we generate the table (by replacing the previous one we no longer need)
        pairs <- pairs[, data.table(
            copy = rep(copy, len),
            pID = rep(pID, len),
            explanatoryHitGroup
        )]

        # we don't need to keep rows involving subjects that belong to the processed hit group
        pairs <- pairs[explanatoryHitGroup != focalHitGroup]

        # for each copy of focalHitGroup, we retain only the most similar subject per
        # explanatoryHitGroup, as we don't need to retain more (and this saves memory)
        setorder(pairs, -pID) # for this we place homologies with highest pID on top
        pairs <- pairs[!duplicated(data.table(copy, explanatoryHitGroup))]

        # we now add the focalHitGroup id to the table. Doing it earlier would have increased the memory footprint.
        pairs[, focalHitGroup := focalHitGroup]
        cat(".") # progess indicator
        pairs
    }

    # applies the above for all hit groups of the super family
    hitGroupPairs <- Map(
        f = hitGroupLinks,
        copies = copyPerHitGroup,
        focalHitGroup = as.integer(names(copyPerHitGroup))
    )

    # we write this table for safety, as it is returned by the function
    writeT(
        data = rbindlist(hitGroupPairs),
        path = stri_c("TEs/clustering/networks/", superF, ".hitGroupPairCopies.txt")
    )

    rbindlist(hitGroupPairs)
}

# we apply the function for all super families in parallel
res <- mclapply(
    X = names(hitList),
    FUN = hitGroupPairStats,
    mc.cores = nCPUs,
    mc.preschedule = F
)

writeT(
    data = rbindlist(res),
    path = stri_c("TEs/clustering/networks/all.hitGroupPairCopies.txt")
)
